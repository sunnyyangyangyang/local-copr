#!/usr/bin/env python3
#
# Local Copr (lc) - A lightweight local RPM build system
# Copyright (C) 2026 Yuanxi (Sunny) Yang
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
import re
import os
import sys
import argparse
import subprocess
import glob
import shutil
import tempfile
import tarfile
import json
from datetime import datetime

# --- ÈÖçÁΩÆÂ∏∏Èáè ---
tool_name = "lc (Local-Copr)"
CONFIG_FILE = ".lc_config" # Â≠òÂÇ®‰ªìÂ∫ìÈÖçÁΩÆÔºàÂ¶ÇGPG Key IDÔºâ

def parse_size_bytes(size_str):
    """
    Ëß£ÊûêÁ±ª‰ºº '16G', '512M', '1024' ÁöÑÂ≠óÁ¨¶‰∏≤‰∏∫Â≠óËäÇÊï¥Êï∞„ÄÇ
    Â¶ÇÊûúËß£ÊûêÂ§±Ë¥•ÊàñËæìÂÖ•ÊòØÁôæÂàÜÊØîÔºåËøîÂõû None„ÄÇ
    """
    if not size_str:
        return None
    
    units = {
        'K': 1024,
        'M': 1024**2,
        'G': 1024**3,
        'T': 1024**4
    }
    
    s = size_str.upper().strip()
    
    # ÊöÇ‰∏çÂ§ÑÁêÜÁôæÂàÜÊØîÔºàÂõ†‰∏∫‰∏çÁü•ÈÅì Host ÊÄªÂÜÖÂ≠òÔºâÔºåÂ¶ÇÊûúÊòØÁôæÂàÜÊØîÂàôÂøΩÁï• Swap Ëá™Âä®ËÆ°ÁÆó
    if '%' in s:
        return None

    try:
        # Á∫ØÊï∞Â≠óÔºåÈªòËÆ§Âçï‰Ωç bytes
        if s.isdigit():
            return int(s)
            
        # Â∏¶Âçï‰Ωç
        for unit, multiplier in units.items():
            if s.endswith(unit):
                number = float(s[:-1])
                return int(number * multiplier)
    except:
        return None
    
    return None

def run_cmd(cmd, cwd=None, env=None, capture_output=False):
    """Â∞ÅË£Ö subprocess"""
    if not capture_output:
        print(f"[{tool_name}] CMD: {' '.join(cmd)}")
    try:
        if capture_output:
            return subprocess.check_output(cmd, cwd=cwd, env=env, text=True).strip()
        subprocess.run(cmd, cwd=cwd, env=env, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error executing command: {e}")
        sys.exit(1)

def do_init(args):
    """ÂàùÂßãÂåñ‰ªìÂ∫ì"""
    repo_path = os.path.abspath(args.repo)
    gpg_key = args.gpg_key
    
    print(f"[{tool_name}] Initializing repo at: {repo_path}")
    
    if not os.path.exists(repo_path):
        os.makedirs(repo_path)
    
    # 1. ËÆ∞ÂΩïÈÖçÁΩÆ (GPG ID)
    config = {}
    if gpg_key:
        print(f"[{tool_name}] GPG Signing Enabled. Key ID: {gpg_key}")
        config["gpg_key_id"] = gpg_key
        
        # 2. ÂØºÂá∫ÂÖ¨Èí•Âà∞‰ªìÂ∫ìÊ†πÁõÆÂΩï
        pub_key_path = os.path.join(repo_path, "RPM-GPG-KEY-local")
        print(f"-> Exporting public key to {pub_key_path}...")
        with open(pub_key_path, "w") as f:
            subprocess.run(["gpg", "--export", "--armor", gpg_key], stdout=f, check=True)

    # [Êñ∞Â¢û] ‰øùÂ≠ò Rebuild ËÆæÁΩÆ    
    if args.enable_rebuild:
        print(f"[{tool_name}] üîÑ Auto-Rebuild (Chain) Enabled.")
        config["auto_rebuild"] = True
    else:
        config["auto_rebuild"] = False
        
    with open(os.path.join(repo_path, CONFIG_FILE), "w") as f:
        json.dump(config, f)

    # ‰øùÂ≠ò .lc_config
    with open(os.path.join(repo_path, CONFIG_FILE), "w") as f:
        json.dump(config, f)

    # 3. ÂàùÂßãÂåñ Repodata
    run_cmd(["createrepo_c", repo_path])
    if gpg_key:
        sign_repodata(repo_path, gpg_key)
    # 4. ÁîüÊàê .repo Ê®°Êùø
    repo_name = os.path.basename(repo_path)
    if gpg_key:
        # ÂºÄÂêØ GPG Ê£ÄÊü•
        gpg_block = f"""gpgcheck=1
repo_gpgcheck=1
gpgkey=file://{repo_path}/RPM-GPG-KEY-local"""
    else:
        # ÂÖ≥Èó≠ GPG Ê£ÄÊü•
        gpg_block = "gpgcheck=0"

    readme_content = f"""[{repo_name}]
name=Local Copr - {repo_name}
baseurl=file://{repo_path}
enabled=1
{gpg_block}
"""
    readme_path = os.path.join(repo_path, "local.repo")
    with open(readme_path, "w") as f:
        f.write(readme_content)
        
    print(f"[{tool_name}] Success. Repo config template saved to {readme_path}")

def do_delete(args):
    """Âà†Èô§‰ªìÂ∫ì"""
    repo_path = os.path.abspath(args.repo)
    # ÂÆâÂÖ®Ê£ÄÊü• (Áï•Ôºå‰øùÊåÅÂéüÊúâÈÄªËæë)
    forbidden_paths = ["/", "/home", "/usr", "/var", "/etc", os.path.expanduser("~")]
    if repo_path in forbidden_paths:
        sys.exit(1)
    if not os.path.exists(repo_path):
        print(f"Error: Repo {repo_path} does not exist.")
        sys.exit(1)
        
    print(f"!!! WARNING !!! Delete {repo_path}?")
    if input("Type 'yes': ").lower() == "yes":
        shutil.rmtree(repo_path)
        print("Deleted.")

def sign_rpms(repo_path, rpm_files, key_id):
    """ÂØπ RPM Êñá‰ª∂ËøõË°åÁ≠æÂêç"""
    if not rpm_files:
        return
    print(f"--- Signing {len(rpm_files)} RPMs with Key {key_id} ---")
    # ‰ΩøÁî® rpm --addsign
    # Ê≥®ÊÑèÔºöËøôÈúÄË¶Å gpg-agent Â§Ñ‰∫éÊ¥ªÂä®Áä∂ÊÄÅÔºåÂê¶Âàô‰ºöÂºπÂá∫ÂØÜÁ†ÅËæìÂÖ•Ê°ÜÊàñÊä•Èîô
    cmd = ["rpm", "--addsign", "--define", f"_gpg_name {key_id}"] + rpm_files
    run_cmd(cmd)

def sign_repodata(repo_path, key_id):
    """ÂØπ repomd.xml ËøõË°åÁ≠æÂêç"""
    repodata_xml = os.path.join(repo_path, "repodata", "repomd.xml")
    if os.path.exists(repodata_xml):
        print(f"--- Signing repodata with Key {key_id} ---")
        # ÁîüÊàê repomd.xml.asc
        # --yes Ë¶ÜÁõñÊóßÁ≠æÂêç
        cmd = ["gpg", "--detach-sign", "--armor", "--yes", "--default-key", key_id, repodata_xml]
        run_cmd(cmd)

def _bump_spec_release(spec_path):
    """
    ‰øÆÊîπ Spec Êñá‰ª∂ÔºåËøΩÂä†Âü∫‰∫éÊó∂Èó¥Êà≥ÁöÑ Patch Âè∑ÔºåÁ°Æ‰øùÁâàÊú¨ÂçïË∞ÉÈÄíÂ¢û„ÄÇ
    ‰æãÂ¶Ç: Release: 1%{?dist} -> Release: 1.p1700000000%{?dist}
    """
    import time
    import re
    
    try:
        with open(spec_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        new_lines = []
        # ÁîüÊàê‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑÊó∂Èó¥Êà≥ patch Âè∑ (‰æãÂ¶Ç .p1704895000)
        patch_suffix = f".p{int(time.time())}"
        changed = False

        # ÂåπÈÖç Release Ë°åÔºåÂøΩÁï•Â§ßÂ∞èÂÜô
        release_pattern = re.compile(r'^(Release:\s*)(.+?)(%\{\?dist\})?$', re.IGNORECASE)

        for line in lines:
            match = release_pattern.match(line.strip())
            if match and not changed:
                # group(1): "Release: "
                # group(2): "1" Êàñ "1.p12345"
                # group(3): "%{?dist}" Êàñ None
                prefix = match.group(1)
                old_ver = match.group(2).strip()
                dist_macro = match.group(3) if match.group(3) else ""
                
                # Â¶ÇÊûú‰ª•ÂâçÂ∑≤ÁªèbumpËøá (ÂåÖÂê´ .p1...), Êàë‰ª¨ÂéªÊéâÊóßÂêéÁºÄÂÜçÂä†Êñ∞ÁöÑ
                # ËøôÊ†∑‰øùËØÅ git ÈáåÊó†ËÆ∫ÊÄé‰πàÊîπÔºåÊûÑÂª∫Âá∫Êù•ÁöÑÊÄªÊòØÊúÄÊñ∞ÁöÑ
                base_ver = re.sub(r'\.p\d+$', '', old_ver)
                
                new_line = f"{prefix}{base_ver}{patch_suffix}{dist_macro}\n"
                new_lines.append(new_line)
                print(f"[{tool_name}] üÜô Version Bump: {old_ver} -> {base_ver}{patch_suffix}")
                changed = True
            else:
                new_lines.append(line)
        
        with open(spec_path, 'w', encoding='utf-8') as f:
            f.writelines(new_lines)
            
    except Exception as e:
        print(f"[{tool_name}] Warning: Failed to bump spec release: {e}")

def chain(args) :
    
    repo_dir = os.path.abspath(args.torepo)
    print(f"[{tool_name}] ‚õìÔ∏è  Chain Mode Triggered: {args.chain}")
    try:
        with open(args.chain) as f:
            tasks = json.load(f).get('tasks', [])
    except Exception as e:
        print(f"Error loading plan: {e}")
        return False

    total = len(tasks)
    print(f"[{tool_name}] Total tasks in chain: {total}")

    for idx, task in enumerate(tasks):
        pkg_name = task['package']
        print(f"\n[{tool_name}] ‚è© Chain Task ({idx+1}/{total}): {pkg_name}")
        args.source = os.path.join(repo_dir, "forges", pkg_name)
        
        # ÈÄíÂΩíË∞ÉÁî® (Â§çÁî®ÊâÄÊúâÈÄªËæë)
        if not single_build(args):
            print(f"[{tool_name}] ‚ùå Chain broken at {pkg_name}. Stopping.")
            return False # ‰∏≠Êñ≠ÈìæÊù°
        
        # Ê≥®ÊÑèÔºödo_build ÁªìÂ∞æËá™Â∏¶ createrepoÔºåÊâÄ‰ª•ËøôÈáå‰∏çÁî®ÂÜô
        # ‰∏ã‰∏ÄÊ¨°Âæ™ÁéØÊó∂ÔºåRepo Â∑≤ÁªèÊòØÊñ∞ÁöÑ‰∫Ü
        
    print(f"[{tool_name}] üéâ Chain Execution Completed.")
    return True

def single_build(args):
    """ÊâßË°åÊûÑÂª∫ÊµÅÁ®ã"""
    repo_dir = os.path.abspath(args.torepo)
    source_dir_origin = os.path.abspath(args.source)
    pkg_name = os.path.basename(source_dir_origin)
    
    # --- 1. ÂèòÈáèÂàùÂßãÂåñ ---
    target_mem = args.max_mem
    target_jobs = args.jobs
    target_net = args.enable_network
    target_tmp_ssd = args.use_tmp_ssd
    target_ssd = args.use_ssd
    target_extras = []
    
    # ÂàùÂßãÂåñ‰ªìÂ∫ìÂàóË°® (‰ªé CLI ÁªßÊâø)
    # Ê≥®ÊÑèÔºöÊàë‰ª¨Ë¶ÅÁî® list() Â§çÂà∂‰∏Ä‰ªΩÔºåÈò≤Ê≠¢Ê±°ÊüìÂÖ®Â±Ä args ÂØπË±°
    target_addrepo = list(args.addrepo) if args.addrepo else []

    # --- 2. Ëá™Âä®ÂÆö‰ΩçÈÖçÁΩÆÊñá‰ª∂ ---
    conf_path = getattr(args, 'conf', None)
    if not conf_path:
        # ÈªòËÆ§Âéª forges/conf.json Êâæ
        default_conf = os.path.join(repo_dir, "forges", "conf.json")
        if os.path.exists(default_conf):
            conf_path = default_conf
            print(f"[{tool_name}] ‚ÑπÔ∏è  Auto-detected config: {conf_path}")

    # --- 3. ËØªÂèñÂπ∂Â∫îÁî®ÈÖçÁΩÆ ---
    if conf_path and os.path.exists(conf_path):
        try:
            with open(conf_path, 'r') as f:
                # Ëé∑ÂèñÁâπÂÆöÂåÖÁöÑÈÖçÁΩÆ
                full_config = json.load(f)
                p_cfg = full_config.get(pkg_name)
                
                if p_cfg:
                    print(f"[{tool_name}] üéØ Applying config for '{pkg_name}'")
                    # get(key, default) -> ÊúâÂàôË¶ÜÁõñÔºåÊó†Âàô‰øùÊåÅ CLI ÂéüÂÄº
                    if "max_mem" in p_cfg: target_mem = p_cfg["max_mem"]
                    if "jobs" in p_cfg: target_jobs = p_cfg["jobs"]
                    if "enable_network" in p_cfg: target_net = p_cfg["enable_network"]
                    if "use_tmp_ssd" in p_cfg: target_tmp_ssd = p_cfg["use_tmp_ssd"]
                    if "use_ssd" in p_cfg: target_ssd = p_cfg["use_ssd"]
                    if "extra_mock_args" in p_cfg: target_extras = p_cfg["extra_mock_args"]
                    
                    # [ÈáçÁÇπ] ÂêàÂπ∂ addrepo
                    conf_repos = p_cfg.get("addrepo", [])
                    if conf_repos:
                        print(f"[{tool_name}] üì¶ Injecting {len(conf_repos)} extra repos")
                        target_addrepo.extend(conf_repos)
                else:
                    print(f"[{tool_name}] ‚ö†Ô∏è  No config found for '{pkg_name}' in conf.json")
                        
        except Exception as e:
            print(f"[{tool_name}] ‚ö†Ô∏è Config load error: {e}")

    # --- 4. ÂáÜÂ§á Mock ÁéØÂ¢É ---
    # ËØªÂèñ‰ªìÂ∫ìÈÖçÁΩÆÔºåÊ£ÄÊü•ÊòØÂê¶ÂêØÁî® GPG
    gpg_key_id = None
    config_path = os.path.join(repo_dir, CONFIG_FILE)
    if os.path.exists(config_path):
        try:
            with open(config_path, "r") as f:
                cfg = json.load(f)
                gpg_key_id = cfg.get("gpg_key_id")
        except:
            pass

    # Mock Âü∫Á°ÄÂèÇÊï∞
    mock_base_args = ["unbuffer", "mock", "--define", "_changelog_date_check 0"]

    if target_mem:
        if not shutil.which("systemd-run"):
            print(f"[{tool_name}] Error: --max-mem requires 'systemd-run'")
            sys.exit(1)
        
        systemd_props = ["-p", f"MemoryMax={target_mem}"]
        mem_bytes = parse_size_bytes(target_mem)
        if mem_bytes:
            swap_bytes = int(mem_bytes * 0.5)
            systemd_props.extend(["-p", f"MemorySwapMax={swap_bytes}"])
        
        mock_base_args = ["systemd-run", "--scope", "--user", "--quiet"] + systemd_props + mock_base_args    

    if target_net:
        print(f"[{tool_name}] üåê Network access enabled.")
        mock_base_args.append("--enable-network")
    
    if not (target_ssd or target_tmp_ssd):
        mock_base_args.append("--enable-plugin=tmpfs")
    if target_tmp_ssd:
        mock_base_args.append("--enable-plugin=tmpfs_tmponly")
        
    if target_jobs:
        mock_base_args.extend(["--define", f"_smp_mflags -j{target_jobs}"])

    if target_extras:
        mock_base_args.extend(target_extras)

    # Ë∑ØÂæÑÊ£ÄÊü•
    if not os.path.isdir(source_dir_origin): 
        print(f"Error: Source dir {source_dir_origin} not found")
        return False
        
    if not os.path.isdir(repo_dir): 
        print(f"Error: Repo dir {repo_dir} not found")
        return False

    # Á°ÆÂÆö Spec
    spec_file_arg = args.spec
    if spec_file_arg:
        spec_path_origin = os.path.abspath(spec_file_arg)
    else:
        specs = glob.glob(os.path.join(source_dir_origin, "*.spec"))
        if not specs: 
            print("Error: No spec file found")
            return False
        spec_path_origin = specs[0]

    # --- 5. ÂºÄÂßãÊûÑÂª∫ ---
    with tempfile.TemporaryDirectory(prefix="lc-build-") as work_dir:
        build_success = False
        spec_name = os.path.basename(spec_path_origin).replace('.spec','')
        log_source_dir = work_dir 
        rpm_result_dir = None 

        try:
            print(f"[{tool_name}] Preparing sources for {pkg_name}...")
            temp_src_dir = os.path.join(work_dir, "clean_sources")
            shutil.copytree(source_dir_origin, temp_src_dir, dirs_exist_ok=True, 
                            ignore=shutil.ignore_patterns('.git', '.svn'))
            rel_spec_path = os.path.relpath(spec_path_origin, source_dir_origin)
            temp_spec_path = os.path.join(temp_src_dir, rel_spec_path)
            spec_name = os.path.basename(temp_spec_path).replace('.spec','')

            # Spectool
            run_cmd(["spectool", "-g", "-C", temp_src_dir, temp_spec_path], cwd=temp_src_dir)
            
            # Version Bump
            _bump_spec_release(temp_spec_path)

            # SRPM
            srpm_result_dir = os.path.join(work_dir, "srpm_result")
            os.makedirs(srpm_result_dir)
            cmd_srpm = mock_base_args + ["--buildsrpm", "--spec", temp_spec_path, "--sources", temp_src_dir, "--resultdir", srpm_result_dir]
            run_cmd(cmd_srpm)
            src_rpms = glob.glob(os.path.join(srpm_result_dir, "*.src.rpm"))
            if not src_rpms: raise Exception("SRPM creation failed")
            target_srpm = src_rpms[0]

            # RPM
            rpm_result_dir = os.path.join(work_dir, "rpm_result")
            os.makedirs(rpm_result_dir)
            
            # ÁªÑË£ÖÊûÑÂª∫ÂëΩ‰ª§
            cmd_rpm = mock_base_args + ["--rebuild", target_srpm, "--resultdir", rpm_result_dir]
        
            # [ÂÖ≥ÈîÆ] Ê≥®ÂÖ•Êú¨Âú∞ Repo
            cmd_rpm.append(f"--addrepo=file://{repo_dir}")

            # [ÂÖ≥ÈîÆ] Ê≥®ÂÖ•Â§ñÈÉ® Repos (CLI + Conf)
            if target_addrepo:
                print(f"[{tool_name}] üîó Active repositories for build:")
                for repo in target_addrepo:
                    # Â¶ÇÊûúÊòØÊú¨Âú∞Ë∑ØÂæÑÔºåÂøÖÈ°ªËΩ¨‰∏∫ file://
                    if os.path.exists(repo):
                        repo_url = f"file://{os.path.abspath(repo)}"
                    else:
                        repo_url = repo
                    
                    print(f"  -> {repo_url}")
                    cmd_rpm.append(f"--addrepo={repo_url}")
            
            # ÊâßË°å
            run_cmd(cmd_rpm)

            # ‰øùÂ≠òÁªìÊûú
            new_rpms = [] 
            built_rpms = glob.glob(os.path.join(rpm_result_dir, "*.rpm"))
            for rpm in built_rpms:
                if "debuginfo" in rpm or rpm.endswith(".src.rpm"): continue
                dest = shutil.copy2(rpm, repo_dir)
                new_rpms.append(dest)
                print(f"-> Saved RPM: {os.path.basename(rpm)}")

            if gpg_key_id:
                sign_rpms(repo_dir, new_rpms, gpg_key_id)
            
            build_success = True
            log_source_dir = rpm_result_dir

        except Exception as e:
            print(f"[{tool_name}] ‚ùå Build Failed: {e}")
            build_success = False
            log_source_dir = work_dir

        finally:
            try:
                logs_dir = os.path.join(repo_dir, ".build_logs")
                if not os.path.exists(logs_dir): os.makedirs(logs_dir)
                timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                status_str = "SUCCESS" if build_success else "FAILED"
                archive_name = f"{pkg_name}-{status_str}-{timestamp}.tar.gz"
                archive_path = os.path.join(logs_dir, archive_name)
                with tarfile.open(archive_path, "w:gz") as tar:
                    tar.add(log_source_dir, arcname=f"build-logs-{status_str}")
                print(f"[{tool_name}] Log saved: {archive_path}")
            except Exception as log_err:
                print(f"[{tool_name}] Log error: {log_err}")

    if not build_success: return False

    run_cmd(["createrepo_c", "--update", repo_dir])
    if gpg_key_id: sign_repodata(repo_dir, gpg_key_id)
        
    print(f"[{tool_name}] Package '{pkg_name}' done!")
    return True
def do_build(args):
    if args.chain:
        return chain(args)
    else: 
        return single_build(args)

def main():
    parser = argparse.ArgumentParser(description="Local Copr (lc) - Secure Build Tool")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Init
    p_init = subparsers.add_parser("init", help="Init new repo")
    p_init.add_argument("--repo", required=True)
    p_init.add_argument("--gpg-key", help="GPG Key ID to enable signing (e.g. 3AA5C0AD)")
    p_init.add_argument("--enable-rebuild", action="store_true", help="Enable automatic dependency rebuilds")
    p_init.set_defaults(func=do_init)
    p_init.set_defaults(func=do_init)

    # Delete
    p_del = subparsers.add_parser("remove", help="Delete a repo")
    p_del.add_argument("--repo", required=True)
    p_del.set_defaults(func=do_delete)

    # Build
    p_build = subparsers.add_parser("build", help="Build RPM")
    p_build.add_argument("--source", help="Source dir (required unless --chain)")
    p_build.add_argument("--torepo", required=True)
    p_build.add_argument("--spec", help="Specific spec")
    p_build.add_argument("--addrepo", action="append")
    p_build.add_argument("--use-ssd", action="store_true")
    p_build.add_argument("--use-tmp-ssd", action="store_true")
    p_build.add_argument("--jobs", type=int, help="Limit build cores (e.g. 8 to prevent OOM)")
    p_build.add_argument("--enable-network", action="store_true", help="Allow network access during build (default: offline)")
    p_build.add_argument("--max-mem", help="Limit max memory (e.g. 4G, 512M) using systemd-run")
    p_build.add_argument("--chain", help="Path to JSON build plan")
    p_build.add_argument("--conf", help="JSON config file for package-specific args") 
    p_build.set_defaults(func=do_build)

    args = parser.parse_args()
    if args.command == 'build':
        if not args.source and not args.chain:
            parser.error("Argument error: --source is required unless --chain is specified.")    
    args.func(args)

if __name__ == "__main__":
    main()